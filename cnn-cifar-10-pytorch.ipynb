{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset , DataLoader , TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_whole_dataset():\n",
    "    train_labels = []\n",
    "    \n",
    "    for i in range(1,6):\n",
    "        \n",
    "        batch = pd.read_pickle(f\"./cifar10/data_batch_{i}\")\n",
    "        train_labels += batch[\"labels\"]\n",
    "        \n",
    "        try:\n",
    "            train_imgs = np.concatenate((train_imgs , batch[\"data\"].reshape(-1,3,32,32)) , axis = 0)\n",
    "        except:\n",
    "            train_imgs = batch[\"data\"].reshape(-1,3,32,32)\n",
    "    \n",
    "    train_labels = np.array(train_labels)#np.eye(10)[np.array(train_labels)] #\n",
    "    \n",
    "    test_batch = pd.read_pickle(\"./cifar10/test_batch\")\n",
    "    test_labels = np.array(test_batch[\"labels\"]) #np.eye(10)[np.array(test_batch[\"labels\"])] #\n",
    "    test_imgs = test_batch[\"data\"].reshape(-1,3,32,32)\n",
    "    \n",
    "    mean = train_imgs.mean()\n",
    "    std = train_imgs.std()\n",
    "    \n",
    "    train_imgs , test_imgs = (train_imgs - mean)/std , (test_imgs - mean)/std\n",
    "    \n",
    "    train_labels , train_imgs = torch.tensor(train_labels) , torch.tensor(train_imgs)\n",
    "    test_labels , test_imgs = torch.tensor(test_labels) , torch.tensor(test_imgs)\n",
    "    \n",
    "    return train_labels , train_imgs , test_labels , test_imgs , mean , std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels , train_imgs , test_labels , test_imgs , mean , std = loading_whole_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5190,  0.0825, -0.2449,  ..., -0.4631, -0.5254, -0.6502],\n",
       "         [ 0.3007,  0.3787,  0.0669,  ..., -0.3852, -0.6813, -0.7749],\n",
       "         [ 0.3007,  0.2852, -0.0890,  ..., -0.6502, -0.8216, -0.8372],\n",
       "         ...,\n",
       "         [ 0.8463,  0.5502,  0.5190,  ..., -1.2269, -0.9307, -0.4319],\n",
       "         [ 0.6905,  0.5502,  0.5969,  ..., -0.2760,  0.0357,  0.1604],\n",
       "         [ 0.6593,  0.5813,  0.6593,  ...,  0.3475,  0.3475,  0.3475]],\n",
       "\n",
       "        [[ 0.8775,  0.2540, -0.2604,  ..., -0.4007, -0.4787, -0.6190],\n",
       "         [ 0.6125,  0.5034,  0.0669,  ..., -0.3384, -0.6346, -0.7437],\n",
       "         [ 0.5346,  0.3943, -0.0890,  ..., -0.6034, -0.7905, -0.8060],\n",
       "         ...,\n",
       "         [ 0.7216,  0.5190,  0.6125,  ..., -1.3516, -1.0555, -0.5878],\n",
       "         [ 0.5190,  0.4878,  0.6281,  ..., -0.4319, -0.1046,  0.0046],\n",
       "         [ 0.4254,  0.4254,  0.5502,  ...,  0.1916,  0.2072,  0.1916]],\n",
       "\n",
       "        [[ 1.0334,  0.2384, -0.4007,  ..., -0.7749, -0.7749, -0.7905],\n",
       "         [ 0.7528,  0.5190, -0.0422,  ..., -0.6657, -0.9152, -0.9307],\n",
       "         [ 0.6749,  0.4410, -0.1357,  ..., -0.8840, -1.0243, -1.0243],\n",
       "         ...,\n",
       "         [ 0.7060,  0.6125,  0.7684,  ..., -1.3205, -0.9931, -0.4631],\n",
       "         [ 0.1137,  0.1449,  0.3319,  ..., -0.3852, -0.0110,  0.1604],\n",
       "         [-0.0110,  0.0201,  0.1916,  ...,  0.2852,  0.3319,  0.3631]]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_imgs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<matplotlib.image.AxesImage at 0x1ec33180088>, tensor(4, dtype=torch.int32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAPoElEQVR4nO3dX6wc5XnH8e9TMBhhS4Y4GMsmMRCUBKHEIBchgSKa0ojSSAaVRCBacZHGaRWkIpELRFFxql4kLX/EFalTLJyU8ieQFEQjCEGpCBc4GArG1DQx0UliMHYQuNhS/mB4erFj5dg58+7x7uzusd/vRzo6e+bdmXk8Pr8zu/PuvG9kJpKOfH8w6QIkjYdhlyph2KVKGHapEoZdqoRhlypx9DArR8TFwO3AUcC/ZuZX+jzffj5pxDIzZloeg/azR8RRwI+BPwG2A88AV2bm/xTWMezSiLWFfZiX8ecC2zLzp5n5W+BeYPUQ25M0QsOEfRnwi2k/b2+WSZqDhnnPPtNLhd97mR4Ra4A1Q+xHUgeGCft24JRpPy8HXjv4SZm5DlgHvmeXJmmYl/HPAGdExKkRcQxwBfBwN2VJ6trAZ/bM3BcR1wCP0et6W5+ZL3VWmaRODdz1NtDOfBkvjdwout4kHUYMu1QJwy5VwrBLlTDsUiUMu1QJwy5VwrBLlTDsUiUMu1QJwy5VwrBLlTDsUiUMu1QJwy5VwrBLlTDsUiUMu1QJwy5VwrBLlTDsUiUMu1QJwy5VwrBLlTDsUiWGmdiRiJgC9gDvAvsyc1UXRUnq3lBhb/xRZr7RwXYkjZAv46VKDBv2BL4XEc9GxJouCpI0GsO+jD8/M1+LiJOAxyPi5cx8cvoTmj8C/iGQJqyzKZsjYi2wNzNvLjzHKZulEet8yuaIOD4iFu5/DHwK2DLo9iSN1jAv45cA34mI/dv598x8tJOqJHWus5fxs9qZL+Olkev8Zbykw4thlyph2KVKGHapEoZdqkQXN8JUY17L8nfGWoU0GM/sUiUMu1QJwy5VwrBLlTDsUiW8Gn+QGT9U3FjWsnxqBHUsL7S9Wmjz5gO18cwuVcKwS5Uw7FIlDLtUCcMuVcKwS5Ww6+0gpa6rqXEVAawotG0fVxE6onhmlyph2KVKGHapEoZdqoRhlyph2KVK9O16i4j1wKeBXZl5VrPsROA+ej1EU8BnM/Ot0ZV5ZLq2cIvdhjly+1rbuHvg2HuHm9mc2e8CLj5o2fXAE5l5BvBE87OkOaxv2Jv51t88aPFqYEPzeANwacd1SerYoO/Zl2TmDoDm+0ndlSRpFEb+cdmIWAOsGfV+JJUNembfGRFLAZrvu9qemJnrMnNVZq4acF+SOjBo2B8Grm4eXw081E05kkYlMst9PBFxD3AhsBjYCdwE/AdwP/AB4OfAZzLz4It4M21rjnQotTuu0Parjvd1QaHtqY73pW6UBiSdK7/cmTljmX3D3iXDfiDDfvg5nMPuJ+ikShh2qRKGXaqEYZcqYdilShzWV+O9I0v6fV6Nlypn2KVKGHapEoZdqoRhlyph2KVKHBZzva1sWb6gsM4rhbZ9hbZf9i9HjQ8V2la9f+blZ328fZ3d29rbbp6aTUUq8cwuVcKwS5Uw7FIlDLtUCcMuVWLOXI0vDQfVdtX9jcI6ywptc+Vq/OWFtksLl7pvLFy1Xt6y/Lyl7essOL697d7CvrY+UhhY68/+uqVhqn2dTQ+0Nn3/D59vbWtv0XSe2aVKGHapEoZdqoRhlyph2KVKGHapEn273iJiPfBpYFdmntUsWwt8nt/1VN2Qmd8dppDSbCtts6O03GsB9OaparO3fzlj8c9XtLctb+tDA+YXpotZ0rLehwpdefPnt7ft+3J7296nX2htW3B+S5/dovPbN7iqvZ/vxovaO9ju+n77Jje2LF/SvgqLCm27C22lG7OeLrSNy2zO7HcBF8+w/LbMXNl8DRV0SaPXN+yZ+STQd9JGSXPbMO/Zr4mIzRGxPiJO6KwiSSMxaNjvAE6nN67EDuCWtidGxJqI2BQRmwbcl6QODBT2zNyZme9m5nvA14FzC89dl5mrMnPVoEVKGt5AYY+I6bdVXAZs6aYcSaMym663e4ALgcURsR24CbgwIlbSm39+CvjCCGtsdeyA641zaqgVpbbSbW+F/5k/X1xYr63L7sOFjsoF7Tu7btuO1rZtT+9pbVv54L0zN1w11V7H/PY6PvyR9tXOK2xyY0tvXunOx9LYeqXAlO7CLNx02NoV3H50B9M37Jl55QyL7+y4Dkkj5ifopEoYdqkShl2qhGGXKmHYpUpEZo5vZxGd7iwKbaUBJ7d3WUQf/1box7nqm4UVP1LorDn69Pa23S0dQHsL9wGWRpzc2L7e6/e2d2JubOlP+tqj7bu67cb2trsL/T//0t472HqX2ur2VYrdpcsXFvZVuJ3ykcJv/q9blhduRqT0cdTMnDEantmlShh2qRKGXaqEYZcqYdilShh2qRJzZq63krYutkIHFP83wPagdxtfl64qDCrZeocawKIPFhoL/XkLTp55+VThiCwvbG95+y12v36gfRDImx+beXlhrEwe/cdC44Aualm+stCFtrd0u1lbPxmwr/DLs6KwybYeu67D6ZldqoRhlyph2KVKGHapEoZdqsRhcTW+7XpwaSi2nxXaSlfcS1fq225MKPUKFItcVJrAqnDZl6nSRmdevOJ9hXVKvwbtkyEtLtyp8dT47q8qaitxX+GfXDryTxcGMCzdnFKaUur1luWlHqVBeGaXKmHYpUoYdqkShl2qhGGXKmHYpUrMZvqnU4BvACcD7wHrMvP2iDgRuI/eZ/yngM9m5lujKLJtWp1fDri9eYW2BYW2tn9caaK77e33irB8b2ECq6PbRk+jOE1S+7/g1cI6Bfva11tQupFnjni5Zfnuwm9q6Z9VupFnnGMbDmI2Z/Z9wHWZ+VHgPOCLEXEmcD3wRGaeATzR/Cxpjuob9szckZnPNY/3AFvpDd66GtjQPG0DcOmoipQ0vEN6zx4RK4CzgY3AkszcAb0/CMBJXRcnqTuz/rhsRCwAHgSuzcy3I0ofLD1gvTXAmsHKk9SVWZ3ZI2IevaDfnZnfbhbvjIilTftSYNdM62bmusxclZmruihY0mD6hj16p/A7ga2Zeeu0poeBq5vHVwMPdV+epK70nf4pIi4Afgi8SK/rDeAGeu/b7wc+APwc+ExmvtlnW2O7F6rUvVa4calzbWOgAdzzV+1ti0t9gDcVOocWtbyAevk/Czsr3JP1eqGDs7DJsG9mYtqmf+r7nj0zn6L9zs8/HqYoSePjJ+ikShh2qRKGXaqEYZcqYdilSvTteut0Z2PsejscXF5oe6DQdsEAbV/pX86Mcm2hcV97U4xgKifNTlvXm2d2qRKGXaqEYZcqYdilShh2qRKGXaqEXW8q+lKh7arCVHVnDzoaqIZm15tUOcMuVcKwS5Uw7FIlDLtUiVkPJa063VxoO9kr7ocVz+xSJQy7VAnDLlXCsEuVMOxSJQy7VIm+XW8RcQrwDeBketM/rcvM2yNiLfB5YH8HzA2Z+d1RFaq5Z1Oh7biW5b8aRSGaldn0s+8DrsvM5yJiIfBsRDzetN2WmaWuWElzxGzmetsB7Gge74mIrcCyURcmqVuH9J49IlYAZ9ObwRXgmojYHBHrI+KEjmuT1KFZhz0iFgAPAtdm5tvAHcDpwEp6Z/5bWtZbExGbIqL0Fk/SiM1qpJqImAc8AjyWmbfO0L4CeCQzz+qzHUeqOYJcUWh7qGW5F+hGb+CRaiIigDuBrdODHhFLpz3tMmDLsEVKGp2+Z/aIuAD4IfAiva43gBuAK+m9hE9gCvhCczGvtC3P7EeQlQOs83znVehgbWd2B5zUwAz73OSAk1LlDLtUCcMuVcKwS5Uw7FIlvBovHWG8Gi9VzrBLlTDsUiUMu1QJwy5VwrBLlXCuN1Vpxr6pxpHaP+yZXaqEYZcqYdilShh2qRKGXaqEYZcqYdebxmregOu902kVsKDQtqfjfc0VntmlShh2qRKGXaqEYZcqYdilSvS9Gh8R84EngWOb5z+QmTdFxKnAvcCJwHPAX2bmb0dZrAa3sNA2zqvPiwttpbnDuq7/SL3iXjKbM/tvgE9m5sfpzfhzcUScB3wVuC0zzwDeAj43ujIlDatv2LNnb/PjvOYrgU8CDzTLNwCXjqRCSZ2Y1Xv2iDgqIp4HdgGPA68AuzNzX/OU7cCy0ZQoqQuzCntmvpuZK4HlwLnAR2d62kzrRsSaiNgUEZsGL1PSsA7panxm7gb+CzgPWBQR+y/wLQdea1lnXWauysxVwxQqaTh9wx4R74+IRc3j44CLgK3AD4DLm6ddDTw0qiIlDa/v9E8R8TF6F+COovfH4f7M/IeIOI3fdb39N/AXmfmbPts6Uof30iyVboTp+maXWrVN/+Rcbxorwz56zvUmVc6wS5Uw7FIlDLtUCcMuVWLcY9C9Afyseby4+XnSrONAI63jEK64V3E8DsFs6/hgW8NYu94O2HHEprnwqTrrsI5a6vBlvFQJwy5VYpJhXzfBfU9nHQeyjgMdMXVM7D27pPHyZbxUiYmEPSIujoj/jYhtEXH9JGpo6piKiBcj4vlxDq4REesjYldEbJm27MSIeDwiftJ8P2FCdayNiFebY/J8RFwyhjpOiYgfRMTWiHgpIv62WT7WY1KoY6zHJCLmR8SPIuKFpo4vN8tPjYiNzfG4LyKOOaQNZ+ZYv+jdKvsKcBpwDPACcOa462hqmQIWT2C/nwDOAbZMW/ZPwPXN4+uBr06ojrXAl8Z8PJYC5zSPFwI/Bs4c9zEp1DHWYwIEsKB5PA/YSG/AmPuBK5rlXwP+5lC2O4kz+7nAtsz8afaGnr4XWD2BOiYmM58E3jxo8Wp64wbAmAbwbKlj7DJzR2Y+1zzeQ29wlGWM+ZgU6hir7Ol8kNdJhH0Z8ItpP09ysMoEvhcRz0bEmgnVsN+SzNwBvV864KQJ1nJNRGxuXuaP/O3EdBGxAjib3tlsYsfkoDpgzMdkFIO8TiLsM91YP6kugfMz8xzgT4EvRsQnJlTHXHIHcDq9OQJ2ALeMa8cRsQB4ELg2M98e135nUcfYj0kOMchrm0mEfTtwyrSfWwerHLXMfK35vgv4Dr2DOik7I2IpQPN91ySKyMydzS/ae8DXGdMxiYh59AJ2d2Z+u1k89mMyUx2TOibNvg95kNc2kwj7M8AZzZXFY4ArgIfHXUREHB8RC/c/Bj4FbCmvNVIP0xu4EyY4gOf+cDUuYwzHJCICuBPYmpm3Tmsa6zFpq2Pcx2Rkg7yO6wrjQVcbL6F3pfMV4O8mVMNp9HoCXgBeGmcdwD30Xg6+Q++VzueA9wFPAD9pvp84oTq+CbwIbKYXtqVjqOMCei9JNwPPN1+XjPuYFOoY6zEBPkZvENfN9P6w/P2039kfAduAbwHHHsp2/QSdVAk/QSdVwrBLlTDsUiUMu1QJwy5VwrBLlTDsUiUMu1SJ/wcnIuZiGYIL2QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.transpose(train_imgs[3] , (1,2,0))) , train_labels[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA ARGUMENTATION\n",
    "t = [\n",
    "    T.ToPILImage(),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]\n",
    "    \n",
    "transformations = T.Compose(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTensorDataset(Dataset):\n",
    "    \"\"\"TensorDataset with support of transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self, tensors , transforms = None):\n",
    "#     def __init__(self , x , y):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors#.type(torch.float).to(device)\n",
    "        self.transforms = transforms#.type(torch.float).to(device)\n",
    "#         self.x = x.type(torch.float).to(device)\n",
    "#         self.y = y.type(torch.float).to(device)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "\n",
    "        if self.transforms:\n",
    "            x = self.transforms(x)\n",
    "\n",
    "        y = self.tensors[1][index]\n",
    "        \n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n",
    "#         return self.x.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomTensorDataset(tensors = (train_imgs , train_labels) , transforms = transformations)\n",
    "test_set = CustomTensorDataset(tensors = (test_imgs , test_labels) , transforms = transformations)\n",
    "\n",
    "# train_set = CustomTensorDataset(train_imgs , train_labels)\n",
    "# test_set = CustomTensorDataset(test_imgs , test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input type float64 is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-3c874c54e6ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-cf0b7cc80163>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \"\"\"\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[1;34m(pic, mode)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Input type {} is not supported'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Input type float64 is not supported"
     ]
    }
   ],
   "source": [
    "plt.imshow(np.transpose(train_set[32][0] , (1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set , batch_size=512 , shuffle=True)\n",
    "test_loader = DataLoader(test_set , batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img, title=''):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.title(title)\n",
    "    plt.imshow(np.transpose(img.cpu().numpy(), (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "for i,data in enumerate(train_loader):\n",
    "    x , y = data\n",
    "    imshow(torchvision.utils.make_grid(x[:16], 4), title='After Transformations')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3 , 32 , 7 , 1)\n",
    "        self.conv1_bn = nn.BatchNorm2d(32)\n",
    "        self.fc1 = nn.Linear(32*13*13 , 1024)\n",
    "        self.fc2 = nn.Linear(1024 , 10)\n",
    "        \n",
    "    def forward(self , x):\n",
    "        x = self.conv1(x.float())\n",
    "        x = self.conv1_bn(F.relu(x))\n",
    "        x = F.max_pool2d(x , 2)\n",
    "        x = x.view(-1 , 32*13*13)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.log_softmax(x , dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CovNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(7, 7), stride=(1, 1))\n",
       "  (conv1_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=5408, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mynet = CovNet()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mynet.parameters() , lr = 1e-3)\n",
    "mynet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i , (x,y) in enumerate(train_loader):\n",
    "#     x , y = x.to(device) , y.to(device)\n",
    "    print(loss_fn(mynet(x) , y.long()))\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model , train_loader , test_loader , epoch=100 , print_every = 100):\n",
    "    loss = np.zeros(epoch)\n",
    "    model.train()\n",
    "    for e in range(0,epoch):\n",
    "            \n",
    "        for i,(x,y) in enumerate(train_loader):\n",
    "            x , y = x.to(device) , y.to(device)\n",
    "            current_loss = loss_fn(model(x) , y.long())\n",
    "            optimizer.zero_grad()\n",
    "            current_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        loss[e] = current_loss.item()\n",
    "        if e % print_every == 0:\n",
    "            print(f\"Epoch : {e} | test_accuracy = {get_accuracy(model , test_loader)} | train_accuracy = {get_accuracy(model , train_loader)} | loss = {current_loss}\")\n",
    "            plt.cla()\n",
    "            plt.plot(loss)\n",
    "            plt.show()\n",
    "\n",
    "def get_accuracy(model , data_loader):\n",
    "    n_samples , n_correct = 0 , 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,(x,y) in enumerate(data_loader):\n",
    "            x , y = x.to(device) , y.to(device)\n",
    "            y_pred = torch.argmax(model(x) , dim = 1 , keepdims = False)\n",
    "            n_samples += y_pred.size(0)\n",
    "            n_correct += (y == y_pred).sum()\n",
    "    \n",
    "    return n_correct.item()/n_samples            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(mynet , train_loader , test_loader , epoch=32 , print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mynet.load_state_dict(torch.load(\"w3.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CovNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(7, 7), stride=(1, 1))\n",
       "  (conv1_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=5408, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mynet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"doge.jfif\")\n",
    "img = cv2.cvtColor(img , cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1de1e976b88>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAY6klEQVR4nO2de5BdVZXGv5VO59VJOq9Op+mEvMygiCRAExVEmYg8og5BEKFGCmvQWE4oRdEZBqtGptQqdIaXI6CNyRBfvBEi8hCRFKKCaSLkQUBCaCQPko4hJIEkkGTNH/e2NuGsr7tP30dkf7+qrr69v177rD73rD737nXX2ubuEEK89elXbQeEEJVBwS5EIijYhUgEBbsQiaBgFyIRFOxCJEL/vhib2ckArgJQA+AH7n4p+/1+Zl4TaNNnEMMoO9jfchhVGOYGc38n0dizFs3Zv44YTcsxYXdaRN4Tkve5juz2ERsGO1apr7neH6u9fS02b96S+UfnDnYzqwFwNYAPAVgLYImZLXL3JyObGgCjAq3t1+Rg0fMyeiAxep1oe4lWYnYRbRDRVhAtOolA/IyOPZwY3UO06N8zOxiDnXs2H3uu85zkHcSGsSenVupj7c4cbWn5aGjRl5fxMwGsdvc17v4agBsBnNqH+YQQZaQvwd4M4IUuP68tjgkhDkD68p49633Bm95ImNlcAHMBrQYKUU36EuxrAUzo8vN4AOv3/yV3bwXQCgC1ZgfIqpkQ6dGXm+0SANPMbLKZDQBwFoBFpXFLCFFqct/Z3X2PmZ0P4D4UlmwXuPtKZlMLoImJEQ8F40eRVdhG5gkhb2Yogq24Mw7LafeFYPyqIcRoANFKnblgx2LpsDaiTSBalE1g9znmR56LIO/xSvtCuE95dne/G8DdJfJFCFFGtGYmRCIo2IVIBAW7EImgYBciERTsQiRCn1bje8tAA6YGR1y1ILYbnf2Zfwz7c2wzeA5xhGU0WL0FK0CJYDUam4k2nk06i2hRRVGY9ERFC4PosUjKa97RsXY1uRCCghF+6bPUGysMypu3jY7HzlV0ocbH0Z1diERQsAuRCAp2IRJBwS5EIijYhUiEiq7G73DgN0G3qEdviu3ednD2+DSyYj34EeLIFKKNIFq0Gr8153zj8xZjPB1L24LxpT+ObY78P3KsV4g2lGhRSyWyKt0eX46/WhybnYBPED9uD8bZE8PIWwiTp6iFhWd0fuPj6M4uRCIo2IVIBAW7EImgYBciERTsQiSCgl2IRKho6q0fgGGBtvJ3sd3EIPuz8xBysCBdBwCYSjS2S1LECFYhw/qjLc9xMADYEktR3cfVZLr5bK+p6BkD+I4l0aW1NLR4+jPxbDtI0dA9o34faqdk74RUJg7s7ch0ZxciERTsQiSCgl2IRFCwC5EICnYhEkHBLkQi9Cn1ZmbtALaj0Cxrj7u30N8nBxxH7EZPzx6f9CliNIl50kC0sUSLKqW2E5snmCMxUfUaABhJlb0ajL+LzHfH8Fib83liOIloJ2cPf/6o0GIoae+2YFOsnUW84NtN5YGl0Fh/OpamzGPT+/lKkWf/R3dnrROFEAcAehkvRCL0NdgdwC/N7DEzm1sKh4QQ5aGvL+OPdff1ZjYWwP1m9pS7v2GD5eI/gbmlOJgQIj99urO7+/ri900AfgZgZsbvtLp7i7u3sOULIUR5yR3sZlZnZsM6HwM4EcCKUjkmhCgtfXll3QjgZ2bWOc9P3f1eZjAQwORAm/XF2O7wEwJhEjvaQURjpW0s57WSHTCbK2Np0S2x1twca0e9mxzvu8H4k8TmKaLd9Z1YYxnHy76UOfyDx2KT+8h0Pyfa6UTLdz/L0+ixOy0PzI+ovDFODeYOdndfAyDIgAshDjSUehMiERTsQiSCgl2IRFCwC5EICnYhEqGiH2qb9k7DvbcFVUgWpRJAthsbTY5G9ijbtz7W7oqlB74aHOml2Kbx6FibThpfHswaZl5ItKgy7zGyId0LZLooVwoAY4j22ezhl8iHqs8g091KtA8fSkTxV3RnFyIRFOxCJIKCXYhEULALkQgKdiESobIl5rsHA2vema2dwpZU/yEYZ/3FhoTKnR+aF2qrfh3P+O4p2eONR8Y2DaQeZyL7k+eRv+2V12KtLqgaeoSsZ7OV/2eJtpdoQVKA1dz8G9E+QbQx3yRiLvIWtOQtoCm1H9nozi5EIijYhUgEBbsQiaBgFyIRFOxCJIKCXYhEqGjq7eXVr+Ke2UsytVO+lz0OANiVPfz892OTW1fF2gmk3x1+G0t19dnjE0bFNvUTybHmxenBeB8nAHVRKhIAarOHWfu8I4gWpBsBAPcTbUL28PzrY5MFn4q1/yDpTcw5h4hRv8G8aa1S95lj5EnlWWihO7sQiaBgFyIRFOxCJIKCXYhEULALkQgKdiESodvUm5ktAPARAJvc/bDi2CgAN6GwAVM7gDPdnXRiK1B/EHDK5wLxjK8Qw+yyrImfXxOanN7vilDbRqq1RpCeawODdNLIQ2KbQcfFWmFDrAiSemsPKgcBYPTizOE1L8YmU84kea1dS2Ptx7EUtgCcFZs4a0JHCv3Q8aNYa2glhmnRkzv79QBO3m/sIgAPuPs0AA8UfxZCHMB0G+zF/da37Dd8KoCFxccLAcwpsV9CiBKT9z17o7tvAIDi97Glc0kIUQ7KvkBnZnPNrM3M2jpIK3chRHnJG+wbzawJAIrfN0W/6O6t7t7i7i0NbFt0IURZyRvsiwCcW3x8LoA7S+OOEKJcmLvzXzC7AcDxKGz2sxHA1wDcAeBmFFoV/hnAx919/0W8N/F2M48SIQuDcQD4TTA+n9gcF2w/BABrW2KtaWis1RxzVLZw8D8RT5qIdkqoLPpykOcDMOuTPwi1oc9/OltgT/MHSGnePpICXNURa38Kxt9O/Dg8lp4IKg4BYPqlZM6v7AiEGmLEKEfVWzRnUO5JbFpaZqOtbVlm6Vu3eXZ3PzuQPtidrRDiwEGfoBMiERTsQiSCgl2IRFCwC5EICnYhEqHb1FspaTnYvO3LgbicGLYF48cTm08Rbfo1RGQbsEVVaoOIDelgiXjPuSkWNw58jswYPp3bYpvtcSYPwy+Mtfv+JdZODLaco80t/0K0G4m2gmgP7iRiHlg6jF0HzC6Cpfmy52OpN93ZhUgEBbsQiaBgFyIRFOxCJIKCXYhEULALkQgV3esN/QAMDrT3EbsoC0UaPWL6e4k4gmgsfZInTZnvFK95lIgzmWVQOrZxWWjB0muMkxbEmv9zIKwmEw4j2olEe51o4q/ozi5EIijYhUgEBbsQiaBgFyIRFOxCJEJlV+N3IV6NbYjNrrkue/xfryTHero51g7Ju+Ie9S0jfdpAmuGxZeSZ58YaHiFasE8S2T7po6Tv3s+jFm4A3C8gfkS7gT0em2yOGtcBaCMFLeOJG2HjY7bVQd4+c8yOhVo5+tq9Gd3ZhUgEBbsQiaBgFyIRFOxCJIKCXYhEULALkQjdpt7MbAGAjwDY5O6HFccuAfAZAJ37/1zs7nd3e7QaxMUOJDV0dtTrLOpNBwBfmE7Eu4h2DNGmBeO/yjlfbai8d1y8Idad18T7V4392IBsYWrsxSJWdNPONJL7nHRb9vge0mhuzLGxdhzpG0h3B34yGB/OjHLCUmi97yfHiWzi1HFP7uzXAzg5Y/wKd59R/Oo+0IUQVaXbYHf3hwB0u2mjEOLApi/v2c83s2VmtsDMRpbMIyFEWcgb7Nei8C5wBoANAC6LftHM5ppZm5m1ddD3VkKIcpIr2N19o7vvdfd9AK4D6Z3i7q3u3uLuLQ11ed0UQvSVXMFuZk1dfjwNfE8OIcQBQE9SbzegsNHSGDNbC+BrAI43sxkorPO3g5d2/ZXdO4Hngn8Lk6OsFoCRwS5JG8lOTY1tl8TiTpJ2Oe7doTTn+A9kjt+x+OPxfHgwVKz/rNhsbyw1nh7nHN1/ny0MOime8NCjY62Z5Dfrx8VaVCHY/8PE5qlYqgt66wHAvri/HvD1YPwXxOatSbfB7u5nZwzPL4MvQogyok/QCZEICnYhEkHBLkQiKNiFSAQFuxCJUNGGkwNHApM/FogvEsPGYJj1jRxMclevRI0jAeC3obJ8aSC8SJoojovTUyd9ME4B3vfLbfGclGjO9xAbslVW/anEbhLRngvGSQdLrIull0l6jfX7XPe77PFmcn3k2uYL4JVt0R5mQByGpW1EqTu7EImgYBciERTsQiSCgl2IRFCwC5EICnYhEqGye72NHACcGW3MtSa2ixoi1pFqs/7kT7v3hlD64u0/DLVnt301W1h5f3yscfeGUm1N3vRazDNLvpM5Pu3oqNMnEOY2AfC8FukSiqh5AWnO+cxDsRZl8gBgPdGiQrrmDcRoCNHyNmVg6bwoxZZn77g4xac7uxCJoGAXIhEU7EIkgoJdiERQsAuRCJVdjUcdgKOypWVkNT5aOB1wS2zDFlRJLcYV/3tJLL76dPb4ro7scQDAOaFy1z1fInb5OPOE72eO//FhYvSug4m4O5Z2kCzEnmCl/sXFsc3YUbEWtNYDAGwkWug+yySwi+fvF93ZhUgEBbsQiaBgFyIRFOxCJIKCXYhEULALkQg92f5pAoAfAhgHYB+AVne/ysxGAbgJhUZk7QDOdPeX+GxTANycLZHdfWJtCTH6Rqi8PHxRqNV/59p4ytkHZY9PYcURcXO9Wy+Irc64kkxJ+Ol/B8Lq2Kbj4StCreGjUeESgC0k5Ti2IXu8cUZsM3J6rK27Ptb2xVKcZq0nRqwAhfUvzNuDLs98kRafjJ7c2fcAuNDd34FC18J5ZnYogIsAPODu0wA8UPxZCHGA0m2wu/sGd19afLwdwCoAzQBOBbCw+GsLAcwpl5NCiL7Tq/fsZjYJwBEoVJg3uvsGoPAPAcDYUjsnhCgdPQ52MxsK4DYAF7h7j7sumNlcM2szs7aODvaxUiFEOelRsJtZLQqB/hN3v704vNHMmop6E4BNWbbu3uruLe7e0tAQLNoIIcpOt8FuZobCfuyr3P3yLtIiAOcWH58L4M7SuyeEKBU9qXo7FoXSreVm1rnP0cUALgVws5mdB+DPAEhDuE52IN5eiaWvmoLxI4hN/L+n/pvEjBLt/8T+z2W+4AEAnH7pebHZlfN75NH+fOLi7PHlf4ltbvhwrJ11TOw/Jh8fay8/mT0+7tDYZvPiUNq2JTYbzqrewtTbHcToNKKxdNjQnHavB+MsPHu/NVS3we7uDyNOEn6w10cUQlQFfYJOiERQsAuRCAp2IRJBwS5EIijYhUiECjecfA3xPj7DiV3UNZDkYzCQaBOIVku0I3s5DhQ+ixQwMKiiAzCAzMg+mhSl2L5BspRnXU7+5i2kMeMeUuRYH/xtu5fFNlvbQ2no3tgMO4n2QiT8iRixrZp2EY3BUmXRnHls+lb1JoR4C6BgFyIRFOxCJIKCXYhEULALkQgKdiESocKpt92IU2+s6i2qCmJpEFaBtJVog4g2Lhhnp5Gl8uL97XYvj63W/iLWhgQZzFEziRt7ovML/rRsWBlr7/z37PGOS2Ob6NIA0I9Utr38fKzVvz0Q1rXGRs3fjTW8SjR2PbLcYZRMJc8LBgfj8fWrO7sQiaBgFyIRFOxCJIKCXYhEULALkQgVXo1/HcCGQGOFK5ENW3FnhTWspTWbMyoKYadxGNFGxdJhLaE03ttiu2gVnxSL7Hoq1gZNizWMfx8RH84eXkMcYbUp5PLY2R5r9Y2BwC4PkAnpc/1KTruooIjZRCv/cUZAd3YhEkHBLkQiKNiFSAQFuxCJoGAXIhEU7EIkQrepNzObAOCHKFSB7APQ6u5XmdklAD6Dv+WxLnb3u/lsewFEG8AyV6IP97NCgbgXFy9KYHNGlRojiM39RPsD0UgaZyLJQ70Y9OsLe7EBgyYSN2YeQ8QhsbQuSA9GewsBtPHe3sdibTOpNRr3ciCQ7bAwjG1SHBWgALwQJk+WmxVs9b5vXU882APgQndfambDADxmZp1X8BXu/j89mEMIUWV6stfbBhQ/1eLu281sFYDmcjsmhCgtvXrPbmaTUNg69dHi0PlmtszMFpjZyBL7JoQoIT0OdjMbikIT9AvcfRuAawFMBTADhTv/ZYHdXDNrM7O2jo68PbeFEH2lR8FuZrUoBPpP3P12AHD3je6+1933AbgOQGYvFHdvdfcWd29paGBdYIQQ5aTbYDczAzAfwCp3v7zLeFOXXzsNwIrSuyeEKBU9WY0/FsA5AJab2ePFsYsBnG1mM1DYK6cdwGe7n2of4pQScyXa/olt08NSbzU57aK8EXt7ElXsAX9b+siCrIHujs4H4mou1gpvPdH2kArBbUti7ZEghRn1hAPon1xDqu8Om0fmfG8wPmkqMWJVkWOJxq4ddo1Edmz7J5Y+zqYnq/EPI/sq7yanLoQ4kNAn6IRIBAW7EImgYBciERTsQiSCgl2IRKhww8m9AKIyJOZKtAcRs4maQ3YHazi5IxjfnnM+8gnj59bFWrRbEBBncUhFGf2X//Nn8tk1BeNkOqodR7QJRKvPMyHrRsmuK5YqY1p0HTOb3qM7uxCJoGAXIhEU7EIkgoJdiERQsAuRCAp2IRKhCqm3KE3FXIn+J7HUBMs1MVjDyTHBONvjaxzRpsfSqMWxdguZcnT28PO/iU2aoz8LQAcpLGyaTfwYE1QWHh6X3720JK4MG/l1Ura3i5TLDYkqHFm6lF0DrNKy1Kk3Ut0YzhdX3unOLkQiKNiFSAQFuxCJoGAXIhEU7EIkgoJdiESocOqNNZwk+5eFzfqCPBOAfKkOgKc7or3NWJrvHUQjZV5DSGPD+k2h9Orj2eM7SHZw00GxdtAnyTneRZoe9g/OycbY95GfJtdAv5Nibci7Yg1Ls4dfvys2qf0SmW8L0fKm5aK/+1ViE6XYYh90ZxciERTsQiSCgl2IRFCwC5EICnYhEqHb1XgzGwTgIRSWDPsDuNXdv2ZmkwHcCGAUCkue57g7b/zme4FdW7O1QWTTx31BEUQ/toKftxCG2W3OYUOWuplW2xhrdfGK9pCgJmTSsHi63eRQGEj65L3yEtGCYpKpHyAHY+dqI9FyrII/QrIux71A5su7dRjzcWcwzjJDkR99K4TZDWCWu09HYXvmk83sPQC+BeAKd58G4CUA5/VgLiFEleg22L1AZ1vV2uKXA5gF4Nbi+EIAc8rioRCiJPR0f/aa4g6umwDcD+BZAFvdvfM10lrQPTiFENWmR8Hu7nvdfQaA8QBmIvtjYZlvSsxsrpm1mVlbRwd7TyOEKCe9Wo13960AFgN4D4ARZta5wDcewS7f7t7q7i3u3tLQoMV/IapFt9FnZg1mNqL4eDCAEwCsAvAggDOKv3YugDvL5aQQou/0pBCmCcBCM6tB4Z/Dze5+l5k9CeBGM/sGgD8CmN/tTHv3ADuC1NtrJH1VE2zHU8dSXtFWTQAvSmDbNUWFH8yPJTnmA+LiHwCHELOgdqKOZfKOf1ssDiU5u+EjYm1TkB7c+nxsM+LwWIMRjT3XmS84gQ5iApJSBDkf7DkDSS0jiIlcdWrxW+VuZ3P3ZQCOyBhfg8L7dyHE3wF6Ey1EIijYhUgEBbsQiaBgFyIRFOxCJIK5s2qcEh/MrANAZ+5lDOIyskoiP96I/Hgjf29+THT3hiyhosH+hgObtbl7S1UOLj/kR4J+6GW8EImgYBciEaoZ7K1VPHZX5McbkR9v5C3jR9XeswshKotexguRCFUJdjM72cyeNrPVZnZRNXwo+tFuZsvN7HEza6vgcReY2SYzW9FlbJSZ3W9mzxS/k06PZfXjEjNbVzwnj5vZ7Ar4McHMHjSzVWa20sy+UByv6DkhflT0nJjZIDP7g5k9UfTjv4rjk83s0eL5uMnMetdV1d0r+oVCW8xnAUxBoTb0CQCHVtqPoi/tAMZU4bjvB3AkgBVdxr4N4KLi44sAfKtKflwC4MsVPh9NAI4sPh4G4E8ADq30OSF+VPScoFDPO7T4uBbAoyg0jLkZwFnF8e8B+Fxv5q3GnX0mgNXuvsYLradvBHBqFfyoGu7+EN68Q+CpKDTuBCrUwDPwo+K4+wZ3X1p8vB2F5ijNqPA5IX5UFC9Q8iav1Qj2ZgBdG3NXs1mlA/ilmT1mZnOr5EMnje6+AShcdADINq5l53wzW1Z8mV/2txNdMbNJKPRPeBRVPCf7+QFU+JyUo8lrNYI9q+VItVICx7r7kQBOATDPzN5fJT8OJK4FMBWFPQI2ALisUgc2s6EAbgNwgbtvq9Rxe+BHxc+J96HJa0Q1gn0tgAldfg6bVZYbd19f/L4JwM9Q3c47G82sCQCK3+NtX8qIu28sXmj7AFyHCp0TM6tFIcB+4u63F4crfk6y/KjWOSkeu9dNXiOqEexLAEwrriwOAHAWgEWVdsLM6sxsWOdjACcCWMGtysoiFBp3AlVs4NkZXEVOQwXOiZkZCj0MV7n75V2kip6TyI9Kn5OyNXmt1ArjfquNs1FY6XwWwFer5MMUFDIBTwBYWUk/ANyAwsvB11F4pXMegNEAHgDwTPH7qCr58SMAywEsQyHYmirgx/tQeEm6DMDjxa/ZlT4nxI+KnhMAh6PQxHUZCv9Y/rPLNfsHAKsB3AJgYG/m1SfohEgEfYJOiERQsAuRCAp2IRJBwS5EIijYhUgEBbsQiaBgFyIRFOxCJML/A7Uix3FUPCr9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4], device='cuda:0', grad_fn=<NotImplemented>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(mynet(torch.tensor([np.transpose(img , (2,0,1)) for i in range(32)]).to(device)) , dim=1 , keepdims=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.resize(img , (32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = (img - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    3,    10,    20, ..., 49981, 49984, 49990], dtype=int64),)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(train_labels == 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 9, 9,  ..., 9, 1, 1], dtype=torch.int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
